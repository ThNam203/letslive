services:
  letslive_auth:
    build:
      context: ./backend
      dockerfile: ./dockerfiles/auth.Dockerfile
    container_name: letslive_auth
    ports:
      - "7777:7777"
    expose:
      - "7777"
    networks:
      general_network:
    depends_on:
      letslive_auth_db:
        condition: service_healthy
      consul:
        condition: service_healthy
      configserver:
        condition: service_healthy

  letslive_auth_db:
    image: postgres:16.3
    container_name: letslive_auth_db
    shm_size: 64mb
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: letslive_auth
    volumes:
      - postgres_letslive_auth_data:/var/lib/postgresql/data
    networks:
      general_network:
    ports:
      - "9990:5432"
    expose:
      - "5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 10s
      retries: 5

  letslive_user:
    build:
      context: ./backend
      dockerfile: ./dockerfiles/user.Dockerfile
    container_name: letslive_user
    ports:
      - "7778:7778"
    expose:
      - "7778"
    networks:
      general_network:
    depends_on:
      letslive_user_db:
        condition: service_healthy
      consul:
        condition: service_healthy
      configserver:
        condition: service_healthy

  letslive_user_db:
    image: postgres:16.3
    container_name: letslive_user_db
    shm_size: 64mb
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: letslive_user
    volumes:
      - postgres_letslive_user_data:/var/lib/postgresql/data
    networks:
      general_network:
    ports:
      - "9991:5432"
    expose:
      - "5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 10s
      retries: 5

  letslive_transcode:
    build:
      context: ./backend
      dockerfile: ./dockerfiles/transcode.Dockerfile
    container_name: letslive_transcode
    ports:
      - "1935:1935"
      - "7779:7779" 
      - "8889:8889" # expose the webserver
    expose:
      - "1935"
      - "7779"
      - "8889"
    networks:
      general_network:
    depends_on:
      letslive_user_db:
        condition: service_healthy
      consul:
        condition: service_healthy
      configserver:
        condition: service_healthy
    volumes:
      - ./backend/transcode/dockervolume/private:/usr/src/app/transcode/private
      - ./backend/transcode/dockervolume/public:/usr/src/app/transcode/public

  #letslive_ui:
  #  build:
  #    context: ./ui
  #    dockerfile: Dockerfile
  #  container_name: letslive_ui
  #  networks:
  #    general_network:
  #  ports:
  #    - "5000:5000"

  configserver:
    build:
      context: ./backend/configserver
      dockerfile: Dockerfile
    container_name: letslive_configserver
    ports:
      - "8181:8181"
    networks:
      general_network:
    healthcheck:
      test: "curl --fail --silent localhost:8181/actuator/health | grep UP || exit 1"
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    depends_on:
      consul:
        condition: service_healthy

  consul:
    image: hashicorp/consul:latest
    container_name: consul
    ports:
      - "8500:8500"  # Consul UI and API
      - "8600:8600/udp"  # DNS
    expose:
      - "8500"
      - "8600"
    command: agent -dev -client=0.0.0.0 -log-level=error -enable-script-checks -bootstrap-expect=1 -config-dir=/consul/config
    volumes:
      - ./configs/consul.json:/consul/config/consul.json
    networks:
      kong_network:
        ipv4_address: 192.168.1.10
      general_network:
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8500/v1/status/leader"]
      interval: 30s
      timeout: 10s
      retries: 5

  kong_migration:
    image: kong:latest
    command: "kong migrations bootstrap"
    container_name: letslive_kong_migration
    networks:
      - kong_network
    restart: on-failure
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: letslive_kong_db
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: kongpass
    links:
      - letslive_kong_db
    depends_on:
      - letslive_kong_db

  kong:
    image: kong:latest
    container_name: letslive_kong-gateway
    networks:
      kong_network:
      general_network:
    restart: always
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: letslive_kong_db
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: kongpass
      KONG_DNS_RESOLVER: "192.168.1.10:8600,127.0.0.11"
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: "0.0.0.0:8001"
      KONG_ADMIN_LISTEN_SSL: "0.0.0.0:8444"
      KONG_ADMIN_GUI_URL: http://localhost:8002
    ports:
      - "8000:8000"
      - "8443:8443"
      - "8001:8001"
      - "8002:8002"
      - "8444:8444"
    #dns:
    #  - 192.168.1.10:8600
    healthcheck:
      test: ["CMD", "curl", "-f", "http://kong:8001"]
      interval: 5s
      timeout: 2s
      retries: 15
    depends_on:
      - letslive_kong_db
      - kong_migration
      - consul

  letslive_kong_db:
    image: postgres:16.3
    container_name: letslive_kong_db
    shm_size: 64mb
    environment:
      POSTGRES_USER: kong
      POSTGRES_PASSWORD: kongpass
      POSTGRES_DB: kong
    volumes:
      - postgres_letslive_kong_data:/var/lib/postgresql/data
      - ./configs/kong_backup/kongdb_backup_20230816:/kongdb_backup
    networks:
      kong_network:
      general_network:
    depends_on:
      consul:
        condition: service_healthy
    ports:
      - "9989:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U kong"]
      interval: 5s
      timeout: 10s
      retries: 5

  # TODO: remove (or load balancing)? using kong instead for api gateway
  #traefik:
  #  image: traefik:v3.1.7
  #  volumes:
  #    - "./configs/traefik.yaml:/etc/traefik/traefik.yml"
  #  ports:
  #    - "80:80"
  #    - "8999:8080" # traefik dashboard
  #  networks:
  #    general_network:
  #  depends_on:
  #    consul: 
  #      condition: service_healthy

  # serve the ipfs content using the bootstrap node (node has an http route to get files)
  #nginx:
  #  image: nginx:latest
  #  container_name: nginx
  #  ports:
  #    - "8888:80"  # Expose Nginx on port 80 to access externally
  #  volumes:
  #    - ./ipfs-impl/nginx.conf:/etc/nginx/nginx.conf  # nginx config
  #  networks:
  #    ipfs_network:
  #      ipv4_address: 10.5.0.10

  #ipfs_bootstrap:
  #  build:
  #    context: ./ipfs-impl
  #    dockerfile: Dockerfile
  #  container_name: ipfs_bootstrap
  #  command: ["/usr/local/bin/app", "-b"] # run as bootstrap node
  #  ports:
  #    - "4001:4001"
  #    - "8080:8080"
  #  expose:
  #    - "4001"
  #    - "8080"
  #  networks:
  #    ipfs_network:
  #      ipv4_address: 10.5.0.2
  #      aliases:
  #        - ipfs_bootstrap_node

  ##### Use the command below to get the bootstrap ip on host, then replace it with "127.0.0.1" or just set it static like me
  ##### docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
  #
  #ipfs_node_1:
  #  build:
  #    context: ./ipfs-impl
  #    dockerfile: Dockerfile
  #  container_name: ipfs_node_1
  #  # change the address to reflex to yours
  #  command: ["/usr/local/bin/app", "-a", "/ip4/10.5.0.2/tcp/4001/p2p/QmSHeyuLfNPnfG5S1JfJcgxPsVQ23u3JYWnRyYm4vkLGJb"]
  #  networks:
  #    ipfs_network:
  #      #ipv4_address: 10.5.0.3
  #      aliases:
  #        - ipfs_bootstrap_node
  #  expose:
  #    - "4001"
  #  depends_on:
  #    - ipfs_bootstrap

  #ipfs_node_2:
  #  build:
  #    context: ./ipfs-impl
  #    dockerfile: Dockerfile
  #  container_name: ipfs_node_2
  #  # change the address to reflex to yours
  #  command: ["/usr/local/bin/app", "-a", "/ip4/10.5.0.2/tcp/4001/p2p/QmSHeyuLfNPnfG5S1JfJcgxPsVQ23u3JYWnRyYm4vkLGJb"]
  #  networks:
  #    ipfs_network:
  #      #ipv4_address: 10.5.0.4
  #      aliases:
  #        - ipfs_bootstrap_node
  #  expose:
  #    - "4001"
  #  depends_on:
  #    - ipfs_bootstrap

  ## showing bootstrap node metrics
  #prometheus:
  #  image: prom/prometheus:latest
  #  ports:
  #    - "9090:9090"
  #  volumes:
  #    - ./ipfs-impl/prometheus.yml:/etc/prometheus/prometheus.yml
  #  networks:
  #    ipfs_network:
  #  depends_on:
  #    ipfs_bootstrap:
  #      condition: service_started

  #grafana:
  #  image: grafana/grafana:latest
  #  depends_on:
  #    - prometheus
  #  ports:
  #    - "3000:3000"
  #  logging:        # does not work?
  #    driver: none
  #  attach: false
  #  environment:
  #    - GF_AUTH_DISABLE_LOGIN_FORM=true
  #    - GF_AUTH_ANONYMOUS_ENABLED=true
  #    - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
  #  volumes:
  #    - ./ipfs-impl/dashboards/dashboard.yml:/etc/grafana/provisioning/dashboards/main.yml
  #    - ./ipfs-impl/dashboards/datasources.yml:/etc/grafana/provisioning/datasources/prom.yml
  #    - ./ipfs-impl/dashboards/autonat/autonat.json:/var/lib/grafana/dashboards/autonat.json
  #    - ./ipfs-impl/dashboards/autorelay/autorelay.json:/var/lib/grafana/dashboards/autorelay.json
  #    - ./ipfs-impl/dashboards/eventbus/eventbus.json:/var/lib/grafana/dashboards/eventbus.json
  #    - ./ipfs-impl/dashboards/holepunch/holepunch.json:/var/lib/grafana/dashboards/holepunch.json
  #    - ./ipfs-impl/dashboards/identify/identify.json:/var/lib/grafana/dashboards/identify.json
  #    - ./ipfs-impl/dashboards/relaysvc/relaysvc.json:/var/lib/grafana/dashboards/relaysvc.json
  #    - ./ipfs-impl/dashboards/swarm/swarm.json:/var/lib/grafana/dashboards/swarm.json
  #    - ./ipfs-impl/dashboards/resource-manager/resource-manager.json:/var/lib/grafana/dashboards/resource-manager.json
  #  networks:
  #    ipfs_network:

networks:
  ipfs_network:
    driver: bridge
    ipam:
      config:
        - subnet: 10.5.0.0/16

  auth_network:
    driver: bridge

  user_network:
    driver: bridge

  general_network:
    driver: bridge

  kong_network:
    driver: bridge
    ipam:
      config:
        - subnet: 192.168.1.0/24

volumes:
  postgres_letslive_auth_data:
  postgres_letslive_user_data:
  postgres_letslive_kong_data:
